# AGI

Resources I have found useful in my journey of [AGI](https://knowyourmeme.com/memes/shoggoth-with-smiley-face-artificial-intelligence)

# Data
* [chinchilla's wild implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fn1trot6ka6e2)


# LLM models 
See [list](https://en.wikipedia.org/wiki/Large_language_model) on Wikipedia
* [Bloom](https://huggingface.co/bigscience/bloom)
  - [Bloom-LORA](https://github.com/linhduongtuan/BLOOM-LORA)
  - [Petals](https://github.com/bigscience-workshop/petals)
* [Llama](https://github.com/facebookresearch/llama) Fill this [Form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) to get access to the weights
* [redpajama](https://www.together.xyz/blog/redpajama) 
* [stable LLM](https://github.com/stability-AI/stableLM/) (cc license) (see [blog](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models))

# Fine Tuning / Adapters
* [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
* [gpt4all](https://github.com/nomic-ai/gpt4all)
* [PEFT](https://github.com/huggingface/peft)
* [stackllama](https://huggingface.co/blog/stackllama)
* [koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
* [LLaMa-Adapter](https://github.com/zrrskywalker/llama-adapter)
* [OpenChatKit](https://github.com/togethercomputer/OpenChatKit)
* [vicuna](https://vicuna.lmsys.org/)

# Tools
* [dalai](https://github.com/cocktailpeanut/dalai)
* [FUTUREPEDIA](https://www.futurepedia.io/)AI Tools dir updated daily
* [langchain](https://python.langchain.com/en/latest/)
* [llamaindex](https://gpt-index.readthedocs.io/en/latest/index.html)
* [Semantic Kernel](https://github.com/microsoft/semantic-kernel) [hello sk](https://devblogs.microsoft.com/semantic-kernel/hello-world/)

# Vector DBs
* [Chroma](https://www.trychroma.com/)
* [FAISS](https://github.com/facebookresearch/faiss) (were doing this way back)
* [Pinecone](https://www.pinecone.io/)
* [Vectera](https://vectara.com/)
* [Weaviate](https://weaviate.io/)

# Backends
* [deepspeed](https://github.com/microsoft/DeepSpeed)
* [ray](https://www.ray.io/)
* [modal](https://modal.com/)

# General Resources
* [reentry](https://rentry.org/localmodelslinks)
* [Distributed AI Research Institute](https://www.dair-institute.org/)

# Companies
* [startups @builtwithgenai](https://airtable.com/shr6nfE9FOHp17IjG/tblL3ekHZfkm3p6YT)
* [cohere](https://cohere.com/)

# Democratizaters
* [baseten](https://www.baseten.co/about)
* [huggingface](https://huggingface.co/)
* [Stable Diffusion Training with MosaicML](https://github.com/mosaicml/diffusion)

# Reports / news
* [Dont worry about the Vase](https://thezvi.wordpress.com/) also author of [lesswrong](https://www.lesswrong.com/)
* [THE AI INDEX REPORT](https://aiindex.stanford.edu/report/)

# Papers (by no means complete)
* [Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning](https://arxiv.org/pdf/2211.04325.pdf)(2022)
* [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)(2022)
* [SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)
* [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)(2021)
* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)(2023)
* [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf)
* [Choose Your Weapon: Survival Strategies for Depressed AI Academics](https://arxiv.org/pdf/2304.06035.pdf)
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) (2023)
* [What Language Model to Train if You Have One Million GPU Hours?](https://arxiv.org/abs/2210.15424)(2022)

# NLP Courses
* [Mohit Iyyer @UMASS](https://people.cs.umass.edu/~miyyer/cs685/schedule.html)

# Prompt
* [promptbase](https://promptbase.com/)

# People
* [Ian Hogarth](https://www.ianhogarth.com/about)
* 
