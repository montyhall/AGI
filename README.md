# AGI

Resources I have found useful in my journey


# Fine Tuning / Adapters
* [PEFT](https://github.com/huggingface/peft)
* [stackllama](https://huggingface.co/blog/stackllama)
* [koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
* [LLaMa-Adapter](https://github.com/zrrskywalker/llama-adapter)
* [vicuna](https://vicuna.lmsys.org/)

# Tools
* [dalai](https://github.com/cocktailpeanut/dalai)
* [FUTUREPEDIA](https://www.futurepedia.io/)AI Tools dir updated daily
* [langchain](https://python.langchain.com/en/latest/)
* [llamaindex](https://gpt-index.readthedocs.io/en/latest/index.html)
* [Semantic Kernel](https://github.com/microsoft/semantic-kernel) [hello sk](https://devblogs.microsoft.com/semantic-kernel/hello-world/)

# Vector DBs
* [FAISS](https://github.com/facebookresearch/faiss) (were doing this way back)
* [Pinecone](https://www.pinecone.io/)
* [Weaviate](https://weaviate.io/)

# General Resources
* [reentry](https://rentry.org/localmodelslinks)

# Companies
*[startups @builtwithgenai](https://airtable.com/shr6nfE9FOHp17IjG/tblL3ekHZfkm3p6YT)

# Democratizaters
*[baseten](https://www.baseten.co/about)
*[huggingface](https://huggingface.co/)

# Reports / news
* [Dont worry about the Vase](https://thezvi.wordpress.com/) also author of [lesswrong](https://www.lesswrong.com/)
* [THE AI INDEX REPORT](https://aiindex.stanford.edu/report/)

# Papers (by no means complete)
*[Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning](https://arxiv.org/pdf/2211.04325.pdf)(2022)
*[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)(2022)
*
